{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 10_000\n",
    "HIDDEN_SIZE = 100\n",
    "LEARNING_RATE = 4e-6\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, preprocess_fn=None):\n",
    "        self.feature_cols = feature_cols\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "\n",
    "        if preprocess_fn:\n",
    "            self.data = preprocess_fn(df[feature_cols])\n",
    "        else:\n",
    "            self.data = df[feature_cols].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, data_column_size, gen_hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, gen_hidden_size)\n",
    "        self.fc2 = nn.Linear(gen_hidden_size, gen_hidden_size)\n",
    "        self.fc3 = nn.Linear(gen_hidden_size, data_column_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc1(z))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return F.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_column_size, disc_hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(data_column_size, disc_hidden_size)\n",
    "        self.fc2 = nn.Linear(disc_hidden_size, disc_hidden_size)\n",
    "        self.fc3 = nn.Linear(disc_hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return F.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(d_losses, g_losses, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label=\"Discriminator Loss\")\n",
    "    plt.plot(g_losses, label=\"Generator Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"losses_{title}.png\")\n",
    "\n",
    "\n",
    "def convert_to_data(df):\n",
    "    column_data = pd.read_csv(\"min_max_value.csv\")\n",
    "    for row in column_data.itertuples():\n",
    "        label = row.label\n",
    "        max_value = row.max\n",
    "        min_value = row.min\n",
    "        type = row.type\n",
    "        if type == 0:\n",
    "            df[label] = df[label].apply(lambda x: int(x * (max_value - min_value) + min_value))\n",
    "        else:\n",
    "            df[label] = df[label].apply(lambda x: float(x * (max_value - min_value) + min_value))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(gen_model, disc_model, real_data, device):\n",
    "    z = torch.randn(BATCH_SIZE, Z_DIM, device=device)\n",
    "\n",
    "    fake_data = gen_model(z)\n",
    "    fake_output = disc_model(fake_data)\n",
    "    real_output = disc_model(real_data)\n",
    "\n",
    "    d_loss_fake = F.binary_cross_entropy(\n",
    "        fake_output, \n",
    "        torch.zeros_like(fake_output, device=device),\n",
    "    )\n",
    "    d_loss_real = F.binary_cross_entropy(\n",
    "        real_output, \n",
    "        torch.ones_like(real_output, device=device),\n",
    "    )\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    g_loss = F.binary_cross_entropy(\n",
    "        fake_output, \n",
    "        torch.ones_like(fake_output, device=device),\n",
    "    )\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(df):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    columns = df.columns.tolist()\n",
    "    feature_cols = [col for col in columns if col != \"Label\" and col != \"Attempted Category\"]\n",
    "\n",
    "    label_list = df[\"Label\"].unique().tolist()\n",
    "    if len(label_list) != 1:\n",
    "        raise ValueError(f\"Label must be unique: {label_list}\")\n",
    "\n",
    "    dataset = DataFrameDataset(\n",
    "        df,\n",
    "        feature_cols,\n",
    "        preprocess_fn=None\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    data_column_size = len(feature_cols)\n",
    "    gen_hidden_size = disc_hidden_size = HIDDEN_SIZE\n",
    "    gen_lr = disc_lr = LEARNING_RATE\n",
    "\n",
    "    gen_model = Generator(Z_DIM, data_column_size, gen_hidden_size).to(device)\n",
    "    disc_model = Discriminator(data_column_size, disc_hidden_size).to(device)\n",
    "\n",
    "    # PyTorch 2.0+ であれば torch.compile でモデルを高速化\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        gen_model = torch.compile(gen_model)\n",
    "        disc_model = torch.compile(disc_model)\n",
    "\n",
    "    gen_optimizer = optim.Adam(gen_model.parameters(), lr=gen_lr)\n",
    "    disc_optimizer = optim.Adam(\n",
    "        disc_model.parameters(), \n",
    "        lr=disc_lr,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    if len(dataloader) < 10:\n",
    "        total_epochs = 100\n",
    "    elif len(dataloader) < 1000:\n",
    "        total_epochs = 20\n",
    "    else:\n",
    "        total_epochs = 5\n",
    "\n",
    "\n",
    "    print(f\"train gan start\")\n",
    "    for epoch in range(total_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Discriminatorの学習\n",
    "            # ---------------------\n",
    "            disc_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(current_batch_size, Z_DIM, device=device)\n",
    "            fake_data = gen_model(z)\n",
    "            fake_data_clone = fake_data.clone()\n",
    "            fake_data_clone[:,0] = fake_data_clone[:,0] * 65535\n",
    "            fake_data_clone[:,1] = fake_data_clone[:,1] * 17\n",
    "\n",
    "            fake_data_clone[:, 0] = torch.round(fake_data_clone[:, 0])\n",
    "            fake_data_clone[:, 1] = torch.round(fake_data_clone[:, 1])\n",
    "            \n",
    "            fake_data_clone = fake_data_clone.to(device)\n",
    "\n",
    "            # 本物データに対する損失\n",
    "            real_output = disc_model(real_data)\n",
    "            d_loss_real = F.binary_cross_entropy(real_output, torch.ones_like(real_output))\n",
    "\n",
    "            # 偽データに対する損失 (Generatorの勾配は計算しない)\n",
    "            fake_output = disc_model(fake_data_clone.detach())\n",
    "            d_loss_fake = F.binary_cross_entropy(fake_output, torch.zeros_like(fake_output))\n",
    "\n",
    "            # print(d_loss_real)\n",
    "            # print(d_loss_fake)\n",
    "            # input()\n",
    "            # Discriminatorの損失\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Generatorの学習\n",
    "            # -----------------\n",
    "            gen_optimizer.zero_grad()\n",
    "            \n",
    "            # Discriminatorの学習で使った偽データを再利用し、Generatorの勾配を計算\n",
    "            fake_output_for_g = disc_model(fake_data_clone)\n",
    "            g_loss = F.binary_cross_entropy(fake_output_for_g, torch.ones_like(fake_output_for_g))\n",
    "            g_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "\n",
    "            if (i + 1) % 1_000 == 0:\n",
    "                time_elapsed = time.time() - start_time\n",
    "                # print(f\"\\rEpoch {epoch + 1:7d} / {total_epochs:7d} | Step {i + 1:10d} / {len(dataloader):10d} | Time elapsed: {time_elapsed:.2f}s\", end=\"\")\n",
    "                print(f\"\\rEpoch {epoch + 1:10d} / {NUM_EPOCHS:10d} | Step {i + 1:10d} / {len(dataloader):10d} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\", end=\"\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"\\r\" + \" \" * 100, end=\"\")\n",
    "        print(f\"\\rEpoch {epoch + 1:7d} / {total_epochs:7d} | Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "        if (epoch + 1) % 10_000 == 0:\n",
    "            title = f\"Epoch {epoch + 1:7d} / {total_epochs:7d}\"\n",
    "            plot_losses(d_losses, g_losses, title)\n",
    "            print(f\"\\rEpoch {epoch + 1:7d} / {total_epochs:7d} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    label = label_list[0]\n",
    "    torch.save(gen_model.state_dict(), f\"result/gan/gen/gen_model_{label}.pth\")\n",
    "    torch.save(disc_model.state_dict(), f\"result/gan/disc/disc_model_{label}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(label, device):\n",
    "    from collections import OrderedDict\n",
    "    state_dict = torch.load(f\"result/gan/gen/gen_model_{label}.pth\", map_location=device)\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            name = k[len(\"_orig_mod.\"):]\n",
    "            new_state_dict[name] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def convert_to_data(df):\n",
    "    column_data = pd.read_csv(\"min_max_value.csv\")\n",
    "    df[\"Protocol\"] = df[\"Protocol\"].apply(lambda x: int(x * 17))\n",
    "    df[\"Destination Port\"] = df[\"Destination Port\"].apply(lambda x: int(x * 65535))\n",
    "\n",
    "    for row in column_data.itertuples():\n",
    "        label = row.label\n",
    "        max_value = row.max\n",
    "        min_value = row.min\n",
    "        type = row.type\n",
    "        if type == 0:\n",
    "            df[label] = df[label].apply(lambda x: int(x * (max_value - min_value) + min_value))\n",
    "        else:\n",
    "            df[label] = df[label].apply(lambda x: float(x * (max_value - min_value) + min_value))\n",
    "    return df\n",
    "\n",
    "def generate_data(df, max_size=10_000):\n",
    "    columns = df.columns.tolist()\n",
    "    feature_cols = [col for col in columns if col != \"Label\" and col != \"Attempted Category\"]\n",
    "\n",
    "    label_list = df[\"Label\"].unique().tolist()\n",
    "    if len(label_list) != 1:\n",
    "        raise ValueError(f\"Label must be unique: {label_list}\")\n",
    "    label = label_list[0]\n",
    "\n",
    "    data_column_size = len(feature_cols)\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gen_model = Generator(Z_DIM, data_column_size, HIDDEN_SIZE).to(device)\n",
    "    state_dict = load_state_dict(label, device)\n",
    "    gen_model.load_state_dict(state_dict)\n",
    "    \n",
    "    if len(df) < max_size:\n",
    "        z = torch.randn(max_size - len(df), Z_DIM, device=device)\n",
    "        fake_data = gen_model(z)\n",
    "    else:\n",
    "        z = torch.randn(max_size, Z_DIM, device=device)\n",
    "        fake_data = gen_model(z)\n",
    "\n",
    "    df = pd.DataFrame(fake_data.detach().cpu().numpy(), columns=feature_cols)\n",
    "    df = convert_to_data(df)\n",
    "    df[\"Label\"] = label\n",
    "    df[\"Attempted Category\"] = -2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data done\n",
      "train gan start\n",
      "Epoch       1 /       5 | Time: 95.93s                                                              \n",
      "Epoch       2 /       5 | Time: 94.73s                                                              \n",
      "Epoch       3 /       5 | Time: 94.32s                                                              \n",
      "Epoch       4 /       5 | Time: 94.51s                                                              \n",
      "Epoch       5 /       5 | Time: 94.03s                                                              \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory result/gan/gen does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m label_list \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m df_label \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m label_list[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 122\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | D Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | G Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m label \u001b[38;5;241m=\u001b[39m label_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 122\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult/gan/gen/gen_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(disc_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult/gan/disc/disc_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/school/drl_part2/.venv/lib/python3.10/site-packages/torch/serialization.py:964\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    961\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    965\u001b[0m         _save(\n\u001b[1;32m    966\u001b[0m             obj,\n\u001b[1;32m    967\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/school/drl_part2/.venv/lib/python3.10/site-packages/torch/serialization.py:828\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/school/drl_part2/.venv/lib/python3.10/site-packages/torch/serialization.py:792\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    786\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m--> 792\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory result/gan/gen does not exist."
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = os.path.abspath(\"../data_cicids2017/3_final/cicids2017_formated_scaled.csv\")\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "print(\"load data done\")\n",
    "\n",
    "label_list = df[\"Label\"].unique().tolist()\n",
    "\n",
    "df_label = df[df[\"Label\"] == label_list[0]]\n",
    "\n",
    "train_gan(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = sys.argv\n",
    "# if len(args) != 3:\n",
    "#     print(\"Usage: python 004_gan.py <is_train> <is_generate>\")\n",
    "#     sys.exit(1)\n",
    "# else:\n",
    "#     is_train = args[1] == \"y\"\n",
    "#     is_generate = args[2] == \"y\"\n",
    "#     print(f\"is_train: {is_train}, is_generate: {is_generate}\")\n",
    "\n",
    "# pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# TRAIN_PATH = os.path.abspath(\"data_cicids2017/3_final/cicids2017_formated_scaled.csv\")\n",
    "# df = pd.read_csv(TRAIN_PATH)\n",
    "# print(\"load data done\")\n",
    "\n",
    "# label_list = df[\"Label\"].unique().tolist()\n",
    "# if is_train:\n",
    "#     for label in label_list:\n",
    "#         print(f\"start train {label}\")\n",
    "#         df_label = df[df[\"Label\"] == label]\n",
    "#         train_gan(df_label)\n",
    "#         print(f\"finish train {label}\")\n",
    "\n",
    "# if is_generate:\n",
    "#     print(\"start generate\")\n",
    "#     generated_df = pd.DataFrame()\n",
    "#     for label in label_list:\n",
    "#         print(f\"start generate {label}\")\n",
    "#         df_label = df[df[\"Label\"] == label]\n",
    "#         generated_df = pd.concat([generated_df, generate_data(df_label, max_size=100_000)]) if generated_df is not None else generate_data(df_label, max_size=100_000)\n",
    "#         print(f\"finish generate {label}\")\n",
    "#     print(generated_df.head(2))\n",
    "#     print(\"-\" * 100)\n",
    "#     generated_df.to_csv(\n",
    "#         \"data_cicids2017/3_final/cicids2017_generated_gan.csv\",\n",
    "#         index=False,\n",
    "#         chunksize=10_000,\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
